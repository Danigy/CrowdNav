pygame 1.9.6
Hello from the pygame community. https://www.pygame.org/contribute.html
Loading chipmunk for Linux (64bit) [/usr/local/lib/python3.5/dist-packages/pymunk/libchipmunk.so]
Namespace(draw_screen=False, env_config='configs/env.config', policy='multi_human_rl', policy_config='configs/policy.config', pre_train=False, test=False, train_config='configs/train.config', weights=None)
Gym environment created.
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0003445827  |
| ent_coef_loss           | 2.7251534     |
| entropy                 | 1.3254524     |
| episodes                | 100           |
| fps                     | 278           |
| mean 100 episode reward | -0.9          |
| n_updates               | 51826         |
| policy_loss             | -0.19574764   |
| qf1_loss                | 1.6716385e-05 |
| qf2_loss                | 1.9378502e-05 |
| time_elapsed            | 186           |
| total timesteps         | 51926         |
| value_loss              | 2.869025e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00025329317 |
| ent_coef_loss           | 3.0360606     |
| entropy                 | 1.2273555     |
| episodes                | 200           |
| fps                     | 276           |
| mean 100 episode reward | -0.2          |
| n_updates               | 105225        |
| policy_loss             | 0.08747007    |
| qf1_loss                | 0.00033519714 |
| qf2_loss                | 0.00038106352 |
| time_elapsed            | 381           |
| total timesteps         | 105325        |
| value_loss              | 3.4502795e-05 |
-------------------------------------------
------------------------------------------
| current_lr              | 0.001        |
| ent_coef                | 0.000433771  |
| ent_coef_loss           | 1.39078      |
| entropy                 | 1.6762626    |
| episodes                | 300          |
| fps                     | 273          |
| mean 100 episode reward | 0.2          |
| n_updates               | 145143       |
| policy_loss             | 0.045049716  |
| qf1_loss                | 6.092775e-05 |
| qf2_loss                | 6.564174e-05 |
| time_elapsed            | 531          |
| total timesteps         | 145243       |
| value_loss              | 9.801437e-05 |
------------------------------------------
--------------------------------------------
| current_lr              | 0.001          |
| ent_coef                | 0.0005137274   |
| ent_coef_loss           | 0.2816974      |
| entropy                 | 1.6041126      |
| episodes                | 400            |
| fps                     | 273            |
| mean 100 episode reward | 0.9            |
| n_updates               | 173957         |
| policy_loss             | -0.09052555    |
| qf1_loss                | 0.000117248186 |
| qf2_loss                | 7.59383e-05    |
| time_elapsed            | 637            |
| total timesteps         | 174057         |
| value_loss              | 7.233949e-05   |
--------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0005126468  |
| ent_coef_loss           | 2.1535108     |
| entropy                 | 1.8572655     |
| episodes                | 500           |
| fps                     | 273           |
| mean 100 episode reward | 1             |
| n_updates               | 198257        |
| policy_loss             | -0.16463116   |
| qf1_loss                | 0.00067192304 |
| qf2_loss                | 0.00037846665 |
| time_elapsed            | 725           |
| total timesteps         | 198357        |
| value_loss              | 0.00014463064 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00047454916 |
| ent_coef_loss           | -0.35022646   |
| entropy                 | 1.7435603     |
| episodes                | 600           |
| fps                     | 272           |
| mean 100 episode reward | 0.9           |
| n_updates               | 218742        |
| policy_loss             | -0.30359596   |
| qf1_loss                | 7.429364e-05  |
| qf2_loss                | 5.6547033e-05 |
| time_elapsed            | 802           |
| total timesteps         | 218842        |
| value_loss              | 4.8086753e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0005273455  |
| ent_coef_loss           | 2.4794812     |
| entropy                 | 1.6893778     |
| episodes                | 700           |
| fps                     | 272           |
| mean 100 episode reward | 1             |
| n_updates               | 238955        |
| policy_loss             | -0.2623435    |
| qf1_loss                | 7.2937655e-05 |
| qf2_loss                | 6.2627594e-05 |
| time_elapsed            | 877           |
| total timesteps         | 239055        |
| value_loss              | 6.851471e-05  |
-------------------------------------------
--------------------------------------------
| current_lr              | 0.001          |
| ent_coef                | 0.00063540647  |
| ent_coef_loss           | -2.2112353     |
| entropy                 | 1.6961248      |
| episodes                | 800            |
| fps                     | 272            |
| mean 100 episode reward | 0.5            |
| n_updates               | 256569         |
| policy_loss             | -0.115116045   |
| qf1_loss                | 0.00011319635  |
| qf2_loss                | 0.000112154856 |
| time_elapsed            | 940            |
| total timesteps         | 256669         |
| value_loss              | 8.368378e-05   |
--------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0006189964  |
| ent_coef_loss           | -0.3023175    |
| entropy                 | 1.6901026     |
| episodes                | 900           |
| fps                     | 272           |
| mean 100 episode reward | 0.9           |
| n_updates               | 276345        |
| policy_loss             | -0.12039873   |
| qf1_loss                | 0.0008352755  |
| qf2_loss                | 0.0006903814  |
| time_elapsed            | 1013          |
| total timesteps         | 276445        |
| value_loss              | 0.00020782548 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00062780705 |
| ent_coef_loss           | 3.173779      |
| entropy                 | 1.7232789     |
| episodes                | 1000          |
| fps                     | 272           |
| mean 100 episode reward | 0.7           |
| n_updates               | 293357        |
| policy_loss             | -0.18153355   |
| qf1_loss                | 7.881565e-05  |
| qf2_loss                | 0.00010200844 |
| time_elapsed            | 1078          |
| total timesteps         | 293457        |
| value_loss              | 7.623572e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0006385233  |
| ent_coef_loss           | -1.0179024    |
| entropy                 | 1.7914284     |
| episodes                | 1100          |
| fps                     | 272           |
| mean 100 episode reward | 1             |
| n_updates               | 312324        |
| policy_loss             | -0.2348874    |
| qf1_loss                | 0.00022855832 |
| qf2_loss                | 0.00021150112 |
| time_elapsed            | 1147          |
| total timesteps         | 312424        |
| value_loss              | 0.0001781367  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00057499117 |
| ent_coef_loss           | -0.33780086   |
| entropy                 | 1.672396      |
| episodes                | 1200          |
| fps                     | 271           |
| mean 100 episode reward | 0.9           |
| n_updates               | 329943        |
| policy_loss             | -0.2104196    |
| qf1_loss                | 5.853638e-05  |
| qf2_loss                | 9.663553e-05  |
| time_elapsed            | 1213          |
| total timesteps         | 330043        |
| value_loss              | 8.375745e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0007143604  |
| ent_coef_loss           | -3.2530727    |
| entropy                 | 1.8125178     |
| episodes                | 1300          |
| fps                     | 272           |
| mean 100 episode reward | 0.4           |
| n_updates               | 347294        |
| policy_loss             | -0.25454766   |
| qf1_loss                | 8.949784e-05  |
| qf2_loss                | 7.077313e-05  |
| time_elapsed            | 1276          |
| total timesteps         | 347394        |
| value_loss              | 0.00010530032 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0007093865  |
| ent_coef_loss           | -0.779362     |
| entropy                 | 1.7210057     |
| episodes                | 1400          |
| fps                     | 272           |
| mean 100 episode reward | 0.4           |
| n_updates               | 369997        |
| policy_loss             | -0.22025871   |
| qf1_loss                | 0.0002058217  |
| qf2_loss                | 0.00020646554 |
| time_elapsed            | 1360          |
| total timesteps         | 370097        |
| value_loss              | 0.00017016655 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0006225672  |
| ent_coef_loss           | -2.781014     |
| entropy                 | 1.7562456     |
| episodes                | 1500          |
| fps                     | 271           |
| mean 100 episode reward | 0.8           |
| n_updates               | 394267        |
| policy_loss             | -0.09658356   |
| qf1_loss                | 0.00023254115 |
| qf2_loss                | 0.00019679457 |
| time_elapsed            | 1451          |
| total timesteps         | 394367        |
| value_loss              | 0.00023523554 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0004925663  |
| ent_coef_loss           | -0.42824233   |
| entropy                 | 1.7107699     |
| episodes                | 1600          |
| fps                     | 271           |
| mean 100 episode reward | 0.7           |
| n_updates               | 420446        |
| policy_loss             | -0.09329486   |
| qf1_loss                | 0.0025652142  |
| qf2_loss                | 0.0024979878  |
| time_elapsed            | 1550          |
| total timesteps         | 420546        |
| value_loss              | 0.00050424633 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00040812016 |
| ent_coef_loss           | 1.0471141     |
| entropy                 | 1.6924407     |
| episodes                | 1700          |
| fps                     | 270           |
| mean 100 episode reward | 1             |
| n_updates               | 446872        |
| policy_loss             | -0.16981828   |
| qf1_loss                | 0.0024792722  |
| qf2_loss                | 0.0027836724  |
| time_elapsed            | 1649          |
| total timesteps         | 446972        |
| value_loss              | 0.00011696183 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00063436385 |
| ent_coef_loss           | -0.7493686    |
| entropy                 | 1.7827542     |
| episodes                | 1800          |
| fps                     | 270           |
| mean 100 episode reward | 0.9           |
| n_updates               | 472042        |
| policy_loss             | -0.22765794   |
| qf1_loss                | 0.0006536422  |
| qf2_loss                | 0.0006860973  |
| time_elapsed            | 1743          |
| total timesteps         | 472142        |
| value_loss              | 9.262056e-05  |
-------------------------------------------
--------------------------------------------
| current_lr              | 0.001          |
| ent_coef                | 0.0005248204   |
| ent_coef_loss           | 2.6888757      |
| entropy                 | 1.6642754      |
| episodes                | 1900           |
| fps                     | 270            |
| mean 100 episode reward | 1              |
| n_updates               | 496804         |
| policy_loss             | -0.129849      |
| qf1_loss                | 0.000110256486 |
| qf2_loss                | 0.000114536015 |
| time_elapsed            | 1837           |
| total timesteps         | 496904         |
| value_loss              | 7.961307e-05   |
--------------------------------------------
>>>>> End testing <<<<< collision_penalty:_-1.0__energy_cost:_-0.001__nn_layers:_[64__64]__ped_state:_future_position_500000_vpref_0.5
Final weights saved at:  /home/admin/tensorboard_logs/sac_2_collision_penalty:_-1.0__energy_cost:_-0.001__nn_layers:_[64__64]__ped_state:_future_position_500000_vpref_0.5/stable_baselines.pkl
TEST COMMAND: python3 py3_learning.py --test --weights  /home/admin/tensorboard_logs/sac_2_collision_penalty:_-1.0__energy_cost:_-0.001__nn_layers:_[64__64]__ped_state:_future_position_500000_vpref_0.5/stable_baselines.pkl --draw_screen
